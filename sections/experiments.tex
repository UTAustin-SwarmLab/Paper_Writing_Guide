\subsection{Section Objective}
We have now described our problem and what we \textit{claim} is the best solution.
We now need to provide hard quantitative evidence that proves this is true on diverse datasets or experimental domains. For a fair comparison, we need to describe: 

\begin{enumerate}
    \item \textbf{Evaluation Metrics: } What metrics define a good solution to the optimization problem stated in the problem formulation? Typically, this is a low overall cost or loss, but we also should describe individual terms of the loss function. These metrics \textbf{must appear} in a table or clear figure for \textbf{all benchmarks}. 
    \item \textbf{Benchmarks: } Recap the benchmarks and specific variants of your algorithm you will test. 
    \item \textbf{Diverse Experimental Domains: } Allude to each dataset you test your algorithm on in order of sophistication. Ideally, the last experiment should be on a real robot or hardware.
\end{enumerate}

\textbf{Paragraph 1: }
Describe evaluation metrics, recap benchmarks, and hook the reader by alluding to cool experiments. Make sure to reference all the key plots, ideally in a list, so the reader knows what to anticipate.

\textbf{Subsections: }
Each subsection should correspond to a new experimental domain. In the first paragraph, describe the setup and why the experiment is interesting and is a good setup to rigorously test your algorithm. Use the same notation as in the problem statement. 

Next, reference each Figure in order and show why they support your claim that your algorithm is better. Start with \textbf{aggregate metrics over several trials} such as boxplots/barplots of accuracy, control cost etc. Then, start with \textbf{qualitative, illustrative analyses}, such as visualizing timeseries and images. \SC{A good paper always gives the key take-aways and insights rather than simply stating your method is $X \times$ better than benchmarks.} Follow
the same structure and show the exact same metrics across diverse experimental setups. 

\textbf{Limitations: }
In 1-2 sentences, describe key assumptions and cases where you expect your algorithm will not do well. This should be a note to the practitioner and say you will handle them in future work. Reviewers often ask for this. 

\subsection{Guidelines}

\subsubsection{Figure and Caption Guidelines}

All figures should be described and referenced in order in the text. 
Each caption should be descriptive and have a short title in \textbf{bold} describing the key take-away. The caption should describe why your method is better than benchmarks etc. Never have short, obvious captions that describe the plot title, such as ``Plot of Accuracy vs. Latency''.

\subsubsection{Plots}

The plots should be done using a combination of Matplotlib and the Seaborn Packages in python. All raw data to create the plot should be stored and backed up in GIT as a csv. Axes tick marks, legends, and axes labels should be \textit{at least} \textbf{20 point font}.

If your axes titles or legends describe a mathematical quantity, first state the English name and the variable in LaTeX. For example, \textbf{Acceleration $a$}.

The following guidelines are for lines/curves:

\begin{enumerate}
    \item Line width/thickness \textbf{3} or larger.
    \item Different color \textbf{and} line style (dashed, dots, etc.) for people who print out the paper in black and white.
    \item Use a descriptive legend.
\end{enumerate}

